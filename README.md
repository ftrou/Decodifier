![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)

# Decodifier  
### **The Compiler for AI-Generated Software**

**LLMs donâ€™t write code.**  
They write **intent**.  
Decodifier compiles that intent into working software.

---

## ğŸš€ What is Decodifier?

Decodifier is a new layer in the AI stack:

> **A compiler for LLM-generated software.  
> LLMs write specs. Decodifier builds the code.**

Instead of prompting models to generate Python, TypeScript, or SQL directly, LLMs (or humans) produce **small declarative specs**. Decodifier validates them, compiles them, and updates the codebase â€” **without the model ever reading or editing files.**

This creates a **token firewall** between LLMs and codebases:

- LLMs stay at the **architecture & intent** level  
- Decodifier handles the **code**  
- Projects stay consistent, safe, and scalable

---

## ğŸ§© Why this matters

AI coding today is stuck in a chat window:  
LLMs regenerate files, hallucinate imports, and break architecture.

With Decodifier:

| Without Decodifier | With Decodifier |
|-------------------|-----------------|
| â€œWrite a FastAPI endpoint for usersâ€ | ```yaml<br>kind: backend.http_endpoint<br>name: create_user<br>path: "/api/users"<br>method: post``` |
| 200â€“800 tokens of code | **8 lines of intent** |
| LLM must read repo | **No repo access needed** |
| Architecture drifts | Architecture is enforced |
| Code is the medium | **Specs are the medium** |

**Result:**  

LLMs develop features without touching code.
## ğŸ§® Token Efficiency Example

Traditional LLM coding:
â€¢ 4,000â€“20,000 tokens / request
â€¢ Repeated context reload
â€¢ Frequent hallucinations

With Decodifier specs:
â€¢ 50â€“300 tokens / request
â€¢ No file diffing or context reload
â€¢ Zero hallucinated imports

---

## ğŸ—ï¸ What it looks like

**input â†’**

```yaml
# patterns/specs/backend.user.create.yaml
kind: backend.http_endpoint
name: create_user
path: "/api/users"
method: post
request_model: UserCreate
response_model: User

command â†’

curl -X POST "http://localhost:8000/patterns/build" \
  -H "Content-Type: application/json" \
  -d '{"spec_dir": "patterns/specs", "project_root": "."}'

output â†’

backend/api/generated_endpoints.py  âœ“
backend/request_schemas.py          âœ“
backend/response_schemas.py         âœ“
```

LLM never saw or generated these files.
ğŸ›ï¸ Core Concepts
1. Patterns

Reusable architecture definitions.

Examples:

    backend.model

    backend.http_endpoint

    backend.crud

    backend.request_schema

    backend.storage

    agent.llm_chat

    service.queue_worker

2. Specs

Tiny YAML files produced by humans or LLMs.
3. Compiler

Validates â†’ normalizes â†’ generates â†’ wires code.
4. Token Firewall

LLMs do not read or modify source files.
They operate entirely through specs + build results.
ğŸ“‰ Why this saves tokens

LLMs donâ€™t waste compute on:

  -  repo embeddings

  -  code diffs

  -  file rewrites

  -  correcting hallucinated imports

Instead of generating code, they generate intent.

This reduces token usage by 60â€“90% in AI-assisted development.
ğŸ“ˆ Why this matters at scale

If adopted inside a large organization:

  -  Fewer GPUs needed for development workflows

  -  Models donâ€™t need huge context windows for legacy repos

  -  Smaller models can do more work

  -  Architecture becomes enforceable, not optional

  -  At hyperscaler scale, this could represent
    $100Mâ€“$500M/year in net efficiency
    (compute + engineering time), even with partial rollout.

ğŸ“Œ Status
Version	Stage	What it does
v0.1	PROTOTYPE	LLM-safe file read/write + project ops
v0.2	CURRENT	Pattern engine, validator, FastAPI backend generation
v0.3	In Progress	No-Code-for-LLMs: full backend extension without reading code
v1.0	ROADMAP	Pattern packs, DB/CRUD, agents, auth, diff-safe generation
ğŸ¯ v0.3 Mission

  -  A full backend can be extended without the LLM ever reading the generated code.

Milestones:

  -  backend.model generator

  -  backend.crud integration

  -  request/response schema emitters

  -  router auto-mount

  -  test harness generation

This will complete the first end-to-end pattern chain.
ğŸ›¡ï¸ License

To protect the core compiler logic and prevent closed SaaS forks:

AGPL-3.0

This license allows public use, contributions, and research â€”
but requires that improvements remain open if used as a hosted service.

ğŸ’¬ Getting Started
```bash
pip install -r requirements.txt
uvicorn engine.app.main:app --reload
open http://localhost:8000/dashboard
```

Add a spec â†’ click Generate from Specs â†’ watch the backend evolve.
ğŸ“£ Join the Category

  -  Decodifier is the first compiler for AI-generated software.
  -  LLMs donâ€™t need to write code. They need a compiler that does.

If youâ€™re building AI systems and want to collaborate, open an issue or reach out.

This isnâ€™t a tool.
This is a new layer.
ğŸ§  Vision

Software creation becomes:

Architecture â†’ Patterns â†’ Specs â†’ Compiler â†’ Code â†’ Running System

LLMs operate at the architecture tier.
Decodifier handles the rest.

This is how AI development scales.

## ğŸš€ Quickstart

```bash
pip install decodifier
decodifier init myproject
decodifier generate
